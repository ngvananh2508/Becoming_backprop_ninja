{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276cc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3cc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# read in all the word\n",
    "words = open('/home/nguyen-van-anh/Downloads/names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61045121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocab of characters and mapping to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chars = ['.'] + chars\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0c8363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([182667, 3]), Y.shape: torch.Size([182667])\n",
      "X.shape: torch.Size([22726, 3]), Y.shape: torch.Size([22726])\n",
      "X.shape: torch.Size([22753, 3]), Y.shape: torch.Size([22753])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for chr in w + '.':\n",
    "            ix = stoi[chr]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] #crop and append\n",
    "        \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(f\"X.shape: {X.shape}, Y.shape: {Y.shape}\")\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte,  Yte  = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c825c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function will be used later to compare manual gradients to Pytorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item() # exactly equal\n",
    "    app = torch.allclose(dt, t.grad) # appoximately equal\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91da05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embed  = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of MLP\n",
    "\n",
    "# Embedded matrix\n",
    "C = torch.randn((vocab_size, n_embed))\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden)) * (5/3)/((n_embed*block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden                        ) * 0.1 # using b1 just for fun, it's useless because of BN step\n",
    "\n",
    "#Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size)          ) * 0.1\n",
    "b2 = torch.randn(vocab_size                      ) * 0.1\n",
    "\n",
    "# BatchNorm params\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in params))\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7004314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct mini batch\n",
    "batch_size = n = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size, ))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # Batch X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc53ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3298, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the embedded vectors in the block size\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/batch_size * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(batch_size-1)*bndiff2.sum(0, keepdim=True) # Bessel's correction: (dividing by n - 1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "#  Cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values #view the differentiation as how inputs affect the outputs\n",
    "norm_logits = logits - logit_maxes # for numerical stability (avoid overflow)\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv,\n",
    "          bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad() # store gradients for non-leaf tensors\n",
    "loss.backward()\n",
    "loss\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f51abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, probs.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ae9ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.9956],\n",
       "        [0.8143],\n",
       "        [1.1203],\n",
       "        [0.7209],\n",
       "        [0.8679],\n",
       "        [1.0998],\n",
       "        [1.0154],\n",
       "        [1.6981],\n",
       "        [1.4528],\n",
       "        [0.8336],\n",
       "        [0.7213],\n",
       "        [0.9415],\n",
       "        [0.7016],\n",
       "        [0.8336],\n",
       "        [0.8036],\n",
       "        [0.7016],\n",
       "        [0.7712],\n",
       "        [1.5058],\n",
       "        [0.9388],\n",
       "        [0.8336],\n",
       "        [0.7368],\n",
       "        [1.1038],\n",
       "        [0.8336],\n",
       "        [1.1685],\n",
       "        [1.4102],\n",
       "        [1.1337],\n",
       "        [1.2150],\n",
       "        [0.9996],\n",
       "        [1.0440],\n",
       "        [0.6016],\n",
       "        [1.0178],\n",
       "        [1.0618]], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[25],\n",
       "        [20],\n",
       "        [ 4],\n",
       "        [ 4],\n",
       "        [13],\n",
       "        [21],\n",
       "        [22],\n",
       "        [25],\n",
       "        [ 4],\n",
       "        [25],\n",
       "        [13],\n",
       "        [22],\n",
       "        [ 9],\n",
       "        [25],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [22],\n",
       "        [19],\n",
       "        [ 2],\n",
       "        [25],\n",
       "        [10],\n",
       "        [16],\n",
       "        [25],\n",
       "        [19],\n",
       "        [22],\n",
       "        [ 4],\n",
       "        [16],\n",
       "        [ 7],\n",
       "        [20],\n",
       "        [ 0],\n",
       "        [19],\n",
       "        [20]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a6e4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, h.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a136fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnbias.shape, bnraw.shape, h.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d4dcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff.shape, bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa369d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmeani.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75ddcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 30]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, embcat.shape, W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "459a4edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([32, 30]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, embcat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c9afa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape, Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd83f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 4.3655745685100555e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 3.958120942115784e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1/n\n",
    "dprobs = 1/probs * dlogprobs #remember: derivatives have the same size as objects, if not-one dimension is on the right, we can omit the keepdim parameter\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True)\n",
    "dcounts = dprobs * counts_sum_inv\n",
    "dcounts_sum = dcounts_sum_inv * -(counts_sum**-2) \n",
    "dcounts += dcounts_sum * torch.ones_like(counts) \n",
    "dnorm_logits = dcounts * counts\n",
    "dlogit_maxes = -(dnorm_logits * 1).sum(1, keepdim=True)\n",
    "dlogits = dnorm_logits * 1\n",
    "dlogits += dlogit_maxes * F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) \n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0) # not-one dimension is on the right, can omit keepdim argument.\n",
    "dhpreact = dh * (1.0 - h**2)\n",
    "dbngain = (dhpreact * bnraw).sum(0)\n",
    "dbnbias = (dhpreact * 1.0).sum(0)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0)\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "dbnvar = dbnvar_inv * -0.5 * (bnvar + 1e-5)**-1.5 * 1.0\n",
    "dbndiff2 = dbnvar * torch.ones_like(bndiff2) * 1.0/(n-1)\n",
    "dbndiff += dbndiff2 * 2.0 * bndiff\n",
    "dbnmeani = (dbndiff * -1).sum(0)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += dbnmeani * torch.ones_like(hprebn) * 1.0/n \n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[i,j]    # take the index of character Xb[i,j]\n",
    "        dC[ix] += demb[i, j, :] # accumulate the derivative of each character in the vocab\n",
    "\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dd0aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.329834461212158 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29124b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1) # take derivative of loss wrt to logits (softmax function)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n # mean in loss\n",
    "\n",
    "cmp('logits', dlogits, logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eaffe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "hppreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hppreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed22004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc94eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/   2000: 3.6841\n",
      "    100/   2000: 2.5274\n",
      "    200/   2000: 2.6610\n",
      "    300/   2000: 2.7603\n",
      "    400/   2000: 2.5188\n",
      "    500/   2000: 2.4026\n",
      "    600/   2000: 2.8043\n",
      "    700/   2000: 2.1533\n",
      "    800/   2000: 2.3337\n",
      "    900/   2000: 2.8883\n",
      "   1000/   2000: 2.8480\n",
      "   1100/   2000: 2.1141\n",
      "   1200/   2000: 2.5449\n",
      "   1300/   2000: 2.8278\n",
      "   1400/   2000: 2.6437\n",
      "   1500/   2000: 2.5289\n",
      "   1600/   2000: 2.5408\n",
      "   1700/   2000: 2.5385\n",
      "   1800/   2000: 2.1418\n",
      "   1900/   2000: 2.1343\n"
     ]
    }
   ],
   "source": [
    "# Train a NN: a for loop involves forward pass (include selecting batches), backward pass, and update step.\n",
    "# init\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd))\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden)) * (5/3) / ((n_embd*block_size)**0.5) # He initilization for a better parameter\n",
    "b1 = torch.randn(n_hidden) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size)) * 0.1\n",
    "b2 = torch.randn(vocab_size) * 0.1\n",
    "# Batchnorm params\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in params))\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 2000\n",
    "n = batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # Mini-batch construction\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ) )\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    # Batchnorm layer\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar  = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward() # for correctness comparisions\n",
    "    dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "    # Manual backprop\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh backprop\n",
    "    dhpreact = dh * (1.0 - h**2)\n",
    "    # batchnorm backprop\n",
    "    dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "    dbnbias = (dhpreact * 1).sum(0, keepdim=True)\n",
    "    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = (dhprebn * 1).sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k, j]\n",
    "            dC[ix] += demb[k, j]\n",
    "    \n",
    "\n",
    "    \n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "    # Update step\n",
    "    lr = 0.1 if i < 1000 else 0.01 # step learning rate decy\n",
    "    for p, grad in zip(params, grads):\n",
    "        p.data += -lr * grad\n",
    "    \n",
    "\n",
    "    if i % 100 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "876c8c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5663372874259949,\n",
       " 0.5540695786476135,\n",
       " 0.5599709749221802,\n",
       " 0.5487356185913086,\n",
       " 0.5599564909934998,\n",
       " 0.5597522854804993,\n",
       " 0.5349310636520386,\n",
       " 0.5248026251792908,\n",
       " 0.5615686178207397,\n",
       " 0.5380712151527405,\n",
       " 0.5388144850730896,\n",
       " 0.5004662871360779,\n",
       " 0.5586374998092651,\n",
       " 0.5386630892753601,\n",
       " 0.5095565915107727,\n",
       " 0.510406494140625,\n",
       " 0.4986518323421478,\n",
       " 0.4894055724143982,\n",
       " 0.5150602459907532,\n",
       " 0.47069239616394043,\n",
       " 0.5046494007110596,\n",
       " 0.5101689100265503,\n",
       " 0.4799480736255646,\n",
       " 0.5152291655540466,\n",
       " 0.4798921048641205,\n",
       " 0.4868813455104828,\n",
       " 0.5008696913719177,\n",
       " 0.5041429996490479,\n",
       " 0.5305931568145752,\n",
       " 0.4770512580871582,\n",
       " 0.5086319446563721,\n",
       " 0.485880970954895,\n",
       " 0.5150389075279236,\n",
       " 0.4388466775417328,\n",
       " 0.4994053244590759,\n",
       " 0.506272554397583,\n",
       " 0.4663412272930145,\n",
       " 0.49060940742492676,\n",
       " 0.4672057628631592,\n",
       " 0.46189185976982117,\n",
       " 0.46452686190605164,\n",
       " 0.5098555684089661,\n",
       " 0.48640063405036926,\n",
       " 0.4502503573894501,\n",
       " 0.45929089188575745,\n",
       " 0.44344547390937805,\n",
       " 0.5012738704681396,\n",
       " 0.46041393280029297,\n",
       " 0.4971398413181305,\n",
       " 0.4392678141593933,\n",
       " 0.4895364046096802,\n",
       " 0.4643656611442566,\n",
       " 0.46154358983039856,\n",
       " 0.47630953788757324,\n",
       " 0.47430601716041565,\n",
       " 0.4426612854003906,\n",
       " 0.49143195152282715,\n",
       " 0.4579283893108368,\n",
       " 0.44412747025489807,\n",
       " 0.4304095208644867,\n",
       " 0.4602339565753937,\n",
       " 0.4580398499965668,\n",
       " 0.47820180654525757,\n",
       " 0.41916951537132263,\n",
       " 0.48425355553627014,\n",
       " 0.4783177375793457,\n",
       " 0.4348122775554657,\n",
       " 0.45385080575942993,\n",
       " 0.42813625931739807,\n",
       " 0.41201603412628174,\n",
       " 0.44973674416542053,\n",
       " 0.4662128984928131,\n",
       " 0.4132711589336395,\n",
       " 0.4792930781841278,\n",
       " 0.4163966476917267,\n",
       " 0.5078717470169067,\n",
       " 0.4045915901660919,\n",
       " 0.43313536047935486,\n",
       " 0.4450554847717285,\n",
       " 0.41879627108573914,\n",
       " 0.43579986691474915,\n",
       " 0.41458335518836975,\n",
       " 0.41858750581741333,\n",
       " 0.43628594279289246,\n",
       " 0.49998918175697327,\n",
       " 0.45931026339530945,\n",
       " 0.4081053137779236,\n",
       " 0.4139927327632904,\n",
       " 0.47754615545272827,\n",
       " 0.45600852370262146,\n",
       " 0.43013307452201843,\n",
       " 0.42917945981025696,\n",
       " 0.4140773117542267,\n",
       " 0.44468843936920166,\n",
       " 0.3939755856990814,\n",
       " 0.43894100189208984,\n",
       " 0.4162012040615082,\n",
       " 0.44804075360298157,\n",
       " 0.4690071940422058,\n",
       " 0.3932569921016693,\n",
       " 0.4026794731616974,\n",
       " 0.4428999125957489,\n",
       " 0.4657316505908966,\n",
       " 0.4333358108997345,\n",
       " 0.42924806475639343,\n",
       " 0.4308657944202423,\n",
       " 0.4250713288784027,\n",
       " 0.4459192156791687,\n",
       " 0.4621856212615967,\n",
       " 0.45016276836395264,\n",
       " 0.4634997546672821,\n",
       " 0.4378954768180847,\n",
       " 0.46834197640419006,\n",
       " 0.41303133964538574,\n",
       " 0.48614251613616943,\n",
       " 0.4396730363368988,\n",
       " 0.44578462839126587,\n",
       " 0.4679541289806366,\n",
       " 0.45543384552001953,\n",
       " 0.4090844690799713,\n",
       " 0.4504227042198181,\n",
       " 0.4397505223751068,\n",
       " 0.43903830647468567,\n",
       " 0.37974265217781067,\n",
       " 0.4299520254135132,\n",
       " 0.42104730010032654,\n",
       " 0.39781880378723145,\n",
       " 0.382156640291214,\n",
       " 0.42862099409103394,\n",
       " 0.38243913650512695,\n",
       " 0.4371803104877472,\n",
       " 0.44838476181030273,\n",
       " 0.45641764998435974,\n",
       " 0.4617651104927063,\n",
       " 0.4046522378921509,\n",
       " 0.44346240162849426,\n",
       " 0.45439624786376953,\n",
       " 0.41712477803230286,\n",
       " 0.4525211751461029,\n",
       " 0.45673543214797974,\n",
       " 0.43043991923332214,\n",
       " 0.45206350088119507,\n",
       " 0.4700614809989929,\n",
       " 0.40782445669174194,\n",
       " 0.4327830672264099,\n",
       " 0.44860151410102844,\n",
       " 0.3866906464099884,\n",
       " 0.43897417187690735,\n",
       " 0.4586208462715149,\n",
       " 0.3863586187362671,\n",
       " 0.42432737350463867,\n",
       " 0.41874390840530396,\n",
       " 0.4330582022666931,\n",
       " 0.4372979998588562,\n",
       " 0.4821454584598541,\n",
       " 0.4135337471961975,\n",
       " 0.4480917751789093,\n",
       " 0.38644927740097046,\n",
       " 0.34912654757499695,\n",
       " 0.4714708626270294,\n",
       " 0.3968644440174103,\n",
       " 0.42791855335235596,\n",
       " 0.44265004992485046,\n",
       " 0.4497835338115692,\n",
       " 0.4557620882987976,\n",
       " 0.39566171169281006,\n",
       " 0.4067590534687042,\n",
       " 0.4201114773750305,\n",
       " 0.3745609521865845,\n",
       " 0.35472920536994934,\n",
       " 0.43633782863616943,\n",
       " 0.405191570520401,\n",
       " 0.41917452216148376,\n",
       " 0.3922332227230072,\n",
       " 0.43098190426826477,\n",
       " 0.4451020359992981,\n",
       " 0.42682477831840515,\n",
       " 0.3663436770439148,\n",
       " 0.42370331287384033,\n",
       " 0.3962460160255432,\n",
       " 0.40006333589553833,\n",
       " 0.419464111328125,\n",
       " 0.4552009701728821,\n",
       " 0.4161194860935211,\n",
       " 0.3537006080150604,\n",
       " 0.43195825815200806,\n",
       " 0.4182666838169098,\n",
       " 0.44759616255760193,\n",
       " 0.46221742033958435,\n",
       " 0.4426825940608978,\n",
       " 0.37760910391807556,\n",
       " 0.4358670115470886,\n",
       " 0.446000337600708,\n",
       " 0.4012325704097748,\n",
       " 0.4799307882785797,\n",
       " 0.3709706664085388,\n",
       " 0.41047418117523193,\n",
       " 0.38193246722221375,\n",
       " 0.40940409898757935,\n",
       " 0.4054153263568878,\n",
       " 0.42505133152008057,\n",
       " 0.420860230922699,\n",
       " 0.43583008646965027,\n",
       " 0.3848007917404175,\n",
       " 0.4234192967414856,\n",
       " 0.3944322168827057,\n",
       " 0.3839995861053467,\n",
       " 0.39385974407196045,\n",
       " 0.3978641629219055,\n",
       " 0.3928356170654297,\n",
       " 0.44566887617111206,\n",
       " 0.4381328225135803,\n",
       " 0.44186100363731384,\n",
       " 0.3825700283050537,\n",
       " 0.44509801268577576,\n",
       " 0.4350103437900543,\n",
       " 0.3768160939216614,\n",
       " 0.4303433299064636,\n",
       " 0.4180600047111511,\n",
       " 0.38819757103919983,\n",
       " 0.4758404493331909,\n",
       " 0.40112757682800293,\n",
       " 0.41722241044044495,\n",
       " 0.41181638836860657,\n",
       " 0.44412878155708313,\n",
       " 0.4096556305885315,\n",
       " 0.4400012195110321,\n",
       " 0.417888879776001,\n",
       " 0.42481255531311035,\n",
       " 0.4176296293735504,\n",
       " 0.44434285163879395,\n",
       " 0.36129412055015564,\n",
       " 0.41714170575141907,\n",
       " 0.45366591215133667,\n",
       " 0.3795883059501648,\n",
       " 0.40344223380088806,\n",
       " 0.40690112113952637,\n",
       " 0.39911264181137085,\n",
       " 0.4678538739681244,\n",
       " 0.3921865224838257,\n",
       " 0.43596574664115906,\n",
       " 0.39048632979393005,\n",
       " 0.4101291298866272,\n",
       " 0.38079795241355896,\n",
       " 0.3951447904109955,\n",
       " 0.4128071665763855,\n",
       " 0.4287112057209015,\n",
       " 0.42546889185905457,\n",
       " 0.4458250105381012,\n",
       " 0.41156190633773804,\n",
       " 0.3957661986351013,\n",
       " 0.4302579462528229,\n",
       " 0.40550488233566284,\n",
       " 0.4134189188480377,\n",
       " 0.44712358713150024,\n",
       " 0.44455546140670776,\n",
       " 0.4420662224292755,\n",
       " 0.4231906235218048,\n",
       " 0.4171328544616699,\n",
       " 0.43745294213294983,\n",
       " 0.41412997245788574,\n",
       " 0.4315439760684967,\n",
       " 0.4110557734966278,\n",
       " 0.42166444659233093,\n",
       " 0.38056817650794983,\n",
       " 0.4275566637516022,\n",
       " 0.39178213477134705,\n",
       " 0.4213065207004547,\n",
       " 0.3975602388381958,\n",
       " 0.43754661083221436,\n",
       " 0.3676760494709015,\n",
       " 0.3677384555339813,\n",
       " 0.4185541272163391,\n",
       " 0.39116397500038147,\n",
       " 0.3992439806461334,\n",
       " 0.3622930645942688,\n",
       " 0.4556589126586914,\n",
       " 0.38097813725471497,\n",
       " 0.5116251707077026,\n",
       " 0.4097862243652344,\n",
       " 0.39395931363105774,\n",
       " 0.3727205693721771,\n",
       " 0.3990092873573303,\n",
       " 0.3960779011249542,\n",
       " 0.3708847463130951,\n",
       " 0.40145254135131836,\n",
       " 0.3933774530887604,\n",
       " 0.3944949805736542,\n",
       " 0.4569319784641266,\n",
       " 0.39818546175956726,\n",
       " 0.41019266843795776,\n",
       " 0.41599133610725403,\n",
       " 0.4045044481754303,\n",
       " 0.41003236174583435,\n",
       " 0.45075759291648865,\n",
       " 0.36834877729415894,\n",
       " 0.3320348560810089,\n",
       " 0.4545353055000305,\n",
       " 0.40315383672714233,\n",
       " 0.3985016345977783,\n",
       " 0.4409635365009308,\n",
       " 0.4114145338535309,\n",
       " 0.3974874019622803,\n",
       " 0.4528806209564209,\n",
       " 0.4502647817134857,\n",
       " 0.39213138818740845,\n",
       " 0.4223337173461914,\n",
       " 0.42425504326820374,\n",
       " 0.3998304605484009,\n",
       " 0.4390966296195984,\n",
       " 0.362964391708374,\n",
       " 0.38538432121276855,\n",
       " 0.45941856503486633,\n",
       " 0.3589061498641968,\n",
       " 0.4066075384616852,\n",
       " 0.448952317237854,\n",
       " 0.42948633432388306,\n",
       " 0.38592782616615295,\n",
       " 0.4355694651603699,\n",
       " 0.3751654028892517,\n",
       " 0.4659616947174072,\n",
       " 0.40801379084587097,\n",
       " 0.40053677558898926,\n",
       " 0.42793241143226624,\n",
       " 0.3745787739753723,\n",
       " 0.3989163935184479,\n",
       " 0.4028383195400238,\n",
       " 0.4248800575733185,\n",
       " 0.40065833926200867,\n",
       " 0.3631476163864136,\n",
       " 0.4394252300262451,\n",
       " 0.4682261347770691,\n",
       " 0.3944254517555237,\n",
       " 0.41659411787986755,\n",
       " 0.4261692762374878,\n",
       " 0.4323478639125824,\n",
       " 0.39260804653167725,\n",
       " 0.43285825848579407,\n",
       " 0.45852965116500854,\n",
       " 0.41486552357673645,\n",
       " 0.38415250182151794,\n",
       " 0.4711757004261017,\n",
       " 0.38592812418937683,\n",
       " 0.4091225266456604,\n",
       " 0.4235963821411133,\n",
       " 0.4053981602191925,\n",
       " 0.45143598318099976,\n",
       " 0.41847366094589233,\n",
       " 0.4375719726085663,\n",
       " 0.43239879608154297,\n",
       " 0.36639633774757385,\n",
       " 0.38854026794433594,\n",
       " 0.4483221173286438,\n",
       " 0.3906610608100891,\n",
       " 0.398042768239975,\n",
       " 0.4819193482398987,\n",
       " 0.3619169592857361,\n",
       " 0.3456194996833801,\n",
       " 0.39871877431869507,\n",
       " 0.3900322914123535,\n",
       " 0.4404119849205017,\n",
       " 0.3527435064315796,\n",
       " 0.3291839063167572,\n",
       " 0.35982635617256165,\n",
       " 0.42204558849334717,\n",
       " 0.49791350960731506,\n",
       " 0.37255197763442993,\n",
       " 0.37880387902259827,\n",
       " 0.4223986864089966,\n",
       " 0.43265223503112793,\n",
       " 0.4276157319545746,\n",
       " 0.4230552613735199,\n",
       " 0.37575966119766235,\n",
       " 0.44837936758995056,\n",
       " 0.3625994324684143,\n",
       " 0.35818758606910706,\n",
       " 0.4434814453125,\n",
       " 0.4245889186859131,\n",
       " 0.4295060634613037,\n",
       " 0.42678502202033997,\n",
       " 0.4270668625831604,\n",
       " 0.35340023040771484,\n",
       " 0.41939863562583923,\n",
       " 0.38440999388694763,\n",
       " 0.4340307116508484,\n",
       " 0.3845627009868622,\n",
       " 0.45001477003097534,\n",
       " 0.38787761330604553,\n",
       " 0.416669100522995,\n",
       " 0.4329646825790405,\n",
       " 0.3706001937389374,\n",
       " 0.3639061450958252,\n",
       " 0.44203096628189087,\n",
       " 0.3695605397224426,\n",
       " 0.42011022567749023,\n",
       " 0.32591134309768677,\n",
       " 0.4008009731769562,\n",
       " 0.4105496406555176,\n",
       " 0.47518646717071533,\n",
       " 0.34718525409698486,\n",
       " 0.40119031071662903,\n",
       " 0.41565001010894775,\n",
       " 0.352994829416275,\n",
       " 0.4353215992450714,\n",
       " 0.3961786925792694,\n",
       " 0.38200339674949646,\n",
       " 0.41103723645210266,\n",
       " 0.39408621191978455,\n",
       " 0.3555334806442261,\n",
       " 0.4267771542072296,\n",
       " 0.4697726368904114,\n",
       " 0.4110848009586334,\n",
       " 0.38860824704170227,\n",
       " 0.38861748576164246,\n",
       " 0.32990312576293945,\n",
       " 0.42431727051734924,\n",
       " 0.3763960897922516,\n",
       " 0.3777649402618408,\n",
       " 0.4332289695739746,\n",
       " 0.4054752588272095,\n",
       " 0.3944735527038574,\n",
       " 0.4323424994945526,\n",
       " 0.4152040481567383,\n",
       " 0.43455764651298523,\n",
       " 0.4336797595024109,\n",
       " 0.4182945191860199,\n",
       " 0.4446803629398346,\n",
       " 0.41307225823402405,\n",
       " 0.4607178270816803,\n",
       " 0.39525291323661804,\n",
       " 0.4069286286830902,\n",
       " 0.4582280218601227,\n",
       " 0.40915268659591675,\n",
       " 0.4030427634716034,\n",
       " 0.34348708391189575,\n",
       " 0.3943600654602051,\n",
       " 0.40792229771614075,\n",
       " 0.37888044118881226,\n",
       " 0.37986886501312256,\n",
       " 0.3906586468219757,\n",
       " 0.39303621649742126,\n",
       " 0.44562405347824097,\n",
       " 0.3235641419887543,\n",
       " 0.46556979417800903,\n",
       " 0.44091567397117615,\n",
       " 0.42284515500068665,\n",
       " 0.40693363547325134,\n",
       " 0.4052436053752899,\n",
       " 0.4307161867618561,\n",
       " 0.4805343449115753,\n",
       " 0.3669570982456207,\n",
       " 0.32723626494407654,\n",
       " 0.44634878635406494,\n",
       " 0.42247501015663147,\n",
       " 0.40167754888534546,\n",
       " 0.34694528579711914,\n",
       " 0.4163281321525574,\n",
       " 0.3583844304084778,\n",
       " 0.4018650949001312,\n",
       " 0.4272075593471527,\n",
       " 0.4513106644153595,\n",
       " 0.3771473169326782,\n",
       " 0.4180404841899872,\n",
       " 0.4361465573310852,\n",
       " 0.41006869077682495,\n",
       " 0.41965407133102417,\n",
       " 0.4328380227088928,\n",
       " 0.42245855927467346,\n",
       " 0.4282764494419098,\n",
       " 0.4340992867946625,\n",
       " 0.3253927230834961,\n",
       " 0.39820462465286255,\n",
       " 0.33840411901474,\n",
       " 0.4297054409980774,\n",
       " 0.4358697831630707,\n",
       " 0.40614932775497437,\n",
       " 0.4073655903339386,\n",
       " 0.37040501832962036,\n",
       " 0.33117711544036865,\n",
       " 0.3671446740627289,\n",
       " 0.4978142976760864,\n",
       " 0.4443570673465729,\n",
       " 0.39972344040870667,\n",
       " 0.4093517065048218,\n",
       " 0.40819618105888367,\n",
       " 0.39722940325737,\n",
       " 0.40120548009872437,\n",
       " 0.3736150860786438,\n",
       " 0.43303418159484863,\n",
       " 0.4210965931415558,\n",
       " 0.4075062870979309,\n",
       " 0.4326714873313904,\n",
       " 0.40381383895874023,\n",
       " 0.46239766478538513,\n",
       " 0.36915314197540283,\n",
       " 0.3373049795627594,\n",
       " 0.3761196732521057,\n",
       " 0.4488770663738251,\n",
       " 0.35672515630722046,\n",
       " 0.40606874227523804,\n",
       " 0.38068148493766785,\n",
       " 0.37660831212997437,\n",
       " 0.3716852366924286,\n",
       " 0.3837604820728302,\n",
       " 0.3799705505371094,\n",
       " 0.42005422711372375,\n",
       " 0.33576488494873047,\n",
       " 0.4344046115875244,\n",
       " 0.47624799609184265,\n",
       " 0.35749468207359314,\n",
       " 0.35800817608833313,\n",
       " 0.4169861078262329,\n",
       " 0.4720236659049988,\n",
       " 0.4069053530693054,\n",
       " 0.4171713590621948,\n",
       " 0.329921692609787,\n",
       " 0.4007621109485626,\n",
       " 0.3616327941417694,\n",
       " 0.42374318838119507,\n",
       " 0.37503331899642944,\n",
       " 0.3892791271209717,\n",
       " 0.3915148675441742,\n",
       " 0.4388170838356018,\n",
       " 0.37677744030952454,\n",
       " 0.36996543407440186,\n",
       " 0.3959008753299713,\n",
       " 0.43482547998428345,\n",
       " 0.35939106345176697,\n",
       " 0.42585310339927673,\n",
       " 0.38609540462493896,\n",
       " 0.39646321535110474,\n",
       " 0.38771453499794006,\n",
       " 0.43007737398147583,\n",
       " 0.47636327147483826,\n",
       " 0.4114358127117157,\n",
       " 0.42844486236572266,\n",
       " 0.41954854130744934,\n",
       " 0.4333675503730774,\n",
       " 0.4033481180667877,\n",
       " 0.39287692308425903,\n",
       " 0.3767411410808563,\n",
       " 0.38544514775276184,\n",
       " 0.3857530951499939,\n",
       " 0.3573724031448364,\n",
       " 0.42526373267173767,\n",
       " 0.36372947692871094,\n",
       " 0.38206616044044495,\n",
       " 0.3568560481071472,\n",
       " 0.42204296588897705,\n",
       " 0.472214013338089,\n",
       " 0.36995184421539307,\n",
       " 0.37707605957984924,\n",
       " 0.4233052730560303,\n",
       " 0.4082504212856293,\n",
       " 0.4373859167098999,\n",
       " 0.44229602813720703,\n",
       " 0.38953498005867004,\n",
       " 0.47875580191612244,\n",
       " 0.3667573928833008,\n",
       " 0.43471741676330566,\n",
       " 0.3640223741531372,\n",
       " 0.4199417233467102,\n",
       " 0.36451199650764465,\n",
       " 0.4036701023578644,\n",
       " 0.3799987733364105,\n",
       " 0.30682528018951416,\n",
       " 0.3514696955680847,\n",
       " 0.3442886769771576,\n",
       " 0.42331790924072266,\n",
       " 0.3978217840194702,\n",
       " 0.4258449077606201,\n",
       " 0.44522395730018616,\n",
       " 0.4299159348011017,\n",
       " 0.377328097820282,\n",
       " 0.3347914516925812,\n",
       " 0.3745691478252411,\n",
       " 0.3552230894565582,\n",
       " 0.49987176060676575,\n",
       " 0.32384881377220154,\n",
       " 0.44625139236450195,\n",
       " 0.3973773717880249,\n",
       " 0.355558842420578,\n",
       " 0.3647211492061615,\n",
       " 0.40450501441955566,\n",
       " 0.44645556807518005,\n",
       " 0.38251104950904846,\n",
       " 0.4332248866558075,\n",
       " 0.3602619767189026,\n",
       " 0.3864021897315979,\n",
       " 0.43994051218032837,\n",
       " 0.32943177223205566,\n",
       " 0.3945774734020233,\n",
       " 0.3275170624256134,\n",
       " 0.3840620219707489,\n",
       " 0.3830088675022125,\n",
       " 0.3376660645008087,\n",
       " 0.3812622129917145,\n",
       " 0.3928477168083191,\n",
       " 0.4097082316875458,\n",
       " 0.42590615153312683,\n",
       " 0.44781792163848877,\n",
       " 0.350338339805603,\n",
       " 0.3872925937175751,\n",
       " 0.33456501364707947,\n",
       " 0.4630046784877777,\n",
       " 0.42948025465011597,\n",
       " 0.3979102671146393,\n",
       " 0.32011672854423523,\n",
       " 0.42908772826194763,\n",
       " 0.31552526354789734,\n",
       " 0.4237729012966156,\n",
       " 0.41478797793388367,\n",
       " 0.3747686743736267,\n",
       " 0.4105816185474396,\n",
       " 0.4235611855983734,\n",
       " 0.36039936542510986,\n",
       " 0.39901527762413025,\n",
       " 0.39572107791900635,\n",
       " 0.3566025197505951,\n",
       " 0.42739132046699524,\n",
       " 0.4137096405029297,\n",
       " 0.4490644335746765,\n",
       " 0.3784078359603882,\n",
       " 0.37857896089553833,\n",
       " 0.37687140703201294,\n",
       " 0.3624069392681122,\n",
       " 0.39446741342544556,\n",
       " 0.3977746367454529,\n",
       " 0.3470635414123535,\n",
       " 0.43783101439476013,\n",
       " 0.268429160118103,\n",
       " 0.37196531891822815,\n",
       " 0.42039382457733154,\n",
       " 0.3948433995246887,\n",
       " 0.3833542764186859,\n",
       " 0.4576539099216461,\n",
       " 0.4025593400001526,\n",
       " 0.4228076636791229,\n",
       " 0.4400986433029175,\n",
       " 0.35101979970932007,\n",
       " 0.43739596009254456,\n",
       " 0.3723636567592621,\n",
       " 0.4612928330898285,\n",
       " 0.36503344774246216,\n",
       " 0.38578563928604126,\n",
       " 0.3664528429508209,\n",
       " 0.43296563625335693,\n",
       " 0.4956883192062378,\n",
       " 0.3934417963027954,\n",
       " 0.40096065402030945,\n",
       " 0.39058157801628113,\n",
       " 0.42524611949920654,\n",
       " 0.38508760929107666,\n",
       " 0.42475637793540955,\n",
       " 0.42330580949783325,\n",
       " 0.42437291145324707,\n",
       " 0.4026348292827606,\n",
       " 0.39344874024391174,\n",
       " 0.3731781244277954,\n",
       " 0.38235729932785034,\n",
       " 0.45037680864334106,\n",
       " 0.4074922204017639,\n",
       " 0.3450748026371002,\n",
       " 0.41401639580726624,\n",
       " 0.3724740743637085,\n",
       " 0.36862170696258545,\n",
       " 0.40789127349853516,\n",
       " 0.4250638782978058,\n",
       " 0.43350186944007874,\n",
       " 0.3828437328338623,\n",
       " 0.3740825653076172,\n",
       " 0.3697855472564697,\n",
       " 0.3549714684486389,\n",
       " 0.3793288469314575,\n",
       " 0.4212295413017273,\n",
       " 0.3795645236968994,\n",
       " 0.4284966289997101,\n",
       " 0.3959351181983948,\n",
       " 0.40132373571395874,\n",
       " 0.43964850902557373,\n",
       " 0.4132232666015625,\n",
       " 0.4275795519351959,\n",
       " 0.4383966028690338,\n",
       " 0.3475496470928192,\n",
       " 0.38264700770378113,\n",
       " 0.44571202993392944,\n",
       " 0.38263940811157227,\n",
       " 0.42519015073776245,\n",
       " 0.4083276391029358,\n",
       " 0.340970516204834,\n",
       " 0.3509567975997925,\n",
       " 0.40247589349746704,\n",
       " 0.4021984934806824,\n",
       " 0.4019155204296112,\n",
       " 0.44126757979393005,\n",
       " 0.3749953508377075,\n",
       " 0.40161269903182983,\n",
       " 0.34807223081588745,\n",
       " 0.4297840893268585,\n",
       " 0.425566166639328,\n",
       " 0.3330976665019989,\n",
       " 0.3890589773654938,\n",
       " 0.4521198272705078,\n",
       " 0.3782171905040741,\n",
       " 0.4068601131439209,\n",
       " 0.37440425157546997,\n",
       " 0.4148128926753998,\n",
       " 0.41528579592704773,\n",
       " 0.4278123378753662,\n",
       " 0.36566445231437683,\n",
       " 0.4063054919242859,\n",
       " 0.41135480999946594,\n",
       " 0.3948238790035248,\n",
       " 0.40415292978286743,\n",
       " 0.42605599761009216,\n",
       " 0.4040219783782959,\n",
       " 0.4318057596683502,\n",
       " 0.43790510296821594,\n",
       " 0.41865190863609314,\n",
       " 0.41672930121421814,\n",
       " 0.32981669902801514,\n",
       " 0.5048317909240723,\n",
       " 0.343906044960022,\n",
       " 0.389489084482193,\n",
       " 0.4392496943473816,\n",
       " 0.3690361976623535,\n",
       " 0.4009387791156769,\n",
       " 0.38220056891441345,\n",
       " 0.35976991057395935,\n",
       " 0.4569598436355591,\n",
       " 0.40168485045433044,\n",
       " 0.42662313580513,\n",
       " 0.44111236929893494,\n",
       " 0.4468304514884949,\n",
       " 0.311076283454895,\n",
       " 0.37116727232933044,\n",
       " 0.3892334997653961,\n",
       " 0.40952828526496887,\n",
       " 0.47262507677078247,\n",
       " 0.3635883033275604,\n",
       " 0.438827246427536,\n",
       " 0.39349454641342163,\n",
       " 0.3827470541000366,\n",
       " 0.45286092162132263,\n",
       " 0.3196500241756439,\n",
       " 0.4071717858314514,\n",
       " 0.45487722754478455,\n",
       " 0.3858635425567627,\n",
       " 0.33922380208969116,\n",
       " 0.3545985221862793,\n",
       " 0.4168921411037445,\n",
       " 0.34436678886413574,\n",
       " 0.3110191822052002,\n",
       " 0.37365633249282837,\n",
       " 0.3984839916229248,\n",
       " 0.32576435804367065,\n",
       " 0.4353349804878235,\n",
       " 0.3066183924674988,\n",
       " 0.3965447247028351,\n",
       " 0.4185558259487152,\n",
       " 0.40461620688438416,\n",
       " 0.3930487036705017,\n",
       " 0.3970046937465668,\n",
       " 0.40706974267959595,\n",
       " 0.4092237651348114,\n",
       " 0.3041797876358032,\n",
       " 0.4294991195201874,\n",
       " 0.46210160851478577,\n",
       " 0.4125783443450928,\n",
       " 0.421502947807312,\n",
       " 0.28189361095428467,\n",
       " 0.43557509779930115,\n",
       " 0.4013843834400177,\n",
       " 0.45385828614234924,\n",
       " 0.41744911670684814,\n",
       " 0.3603065013885498,\n",
       " 0.38049036264419556,\n",
       " 0.4289425015449524,\n",
       " 0.39406266808509827,\n",
       " 0.4082871675491333,\n",
       " 0.3745977580547333,\n",
       " 0.35379406809806824,\n",
       " 0.35600775480270386,\n",
       " 0.4076427221298218,\n",
       " 0.41168999671936035,\n",
       " 0.3806650936603546,\n",
       " 0.37380942702293396,\n",
       " 0.3521382808685303,\n",
       " 0.39841604232788086,\n",
       " 0.3333863914012909,\n",
       " 0.3660881221294403,\n",
       " 0.38147875666618347,\n",
       " 0.3817989230155945,\n",
       " 0.3498462736606598,\n",
       " 0.3918701410293579,\n",
       " 0.4375980794429779,\n",
       " 0.3597760498523712,\n",
       " 0.38796305656433105,\n",
       " 0.3461664617061615,\n",
       " 0.32834944128990173,\n",
       " 0.36803779006004333,\n",
       " 0.3623192012310028,\n",
       " 0.37964165210723877,\n",
       " 0.37383827567100525,\n",
       " 0.3441818058490753,\n",
       " 0.41322770714759827,\n",
       " 0.44877925515174866,\n",
       " 0.3569309413433075,\n",
       " 0.4261351227760315,\n",
       " 0.39341673254966736,\n",
       " 0.38982903957366943,\n",
       " 0.36137738823890686,\n",
       " 0.3683871030807495,\n",
       " 0.41438573598861694,\n",
       " 0.46168074011802673,\n",
       " 0.3864378035068512,\n",
       " 0.39519622921943665,\n",
       " 0.44933444261550903,\n",
       " 0.37954404950141907,\n",
       " 0.43071871995925903,\n",
       " 0.3493324816226959,\n",
       " 0.39123788475990295,\n",
       " 0.3603675663471222,\n",
       " 0.4619204103946686,\n",
       " 0.3164694011211395,\n",
       " 0.3782564401626587,\n",
       " 0.39835336804389954,\n",
       " 0.43370863795280457,\n",
       " 0.38220876455307007,\n",
       " 0.35227927565574646,\n",
       " 0.48742830753326416,\n",
       " 0.3615318536758423,\n",
       " 0.3965620696544647,\n",
       " 0.3988564908504486,\n",
       " 0.43688932061195374,\n",
       " 0.36667096614837646,\n",
       " 0.3703721761703491,\n",
       " 0.41953402757644653,\n",
       " 0.42792901396751404,\n",
       " 0.42608875036239624,\n",
       " 0.32158324122428894,\n",
       " 0.414399117231369,\n",
       " 0.4000733196735382,\n",
       " 0.37384703755378723,\n",
       " 0.3490714728832245,\n",
       " 0.4309190809726715,\n",
       " 0.39698606729507446,\n",
       " 0.40969541668891907,\n",
       " 0.4198916256427765,\n",
       " 0.3741278052330017,\n",
       " 0.35644853115081787,\n",
       " 0.3899870812892914,\n",
       " 0.3464679718017578,\n",
       " 0.38901886343955994,\n",
       " 0.36356979608535767,\n",
       " 0.42797142267227173,\n",
       " 0.39800962805747986,\n",
       " 0.4119325876235962,\n",
       " 0.3809472918510437,\n",
       " 0.40693172812461853,\n",
       " 0.45362988114356995,\n",
       " 0.368174284696579,\n",
       " 0.3675926923751831,\n",
       " 0.3828444480895996,\n",
       " 0.4104965627193451,\n",
       " 0.3696824312210083,\n",
       " 0.36359521746635437,\n",
       " 0.41128504276275635,\n",
       " 0.3499661982059479,\n",
       " 0.3557526171207428,\n",
       " 0.40105533599853516,\n",
       " 0.39537882804870605,\n",
       " 0.4165888726711273,\n",
       " 0.3870183229446411,\n",
       " 0.41250109672546387,\n",
       " 0.37473782896995544,\n",
       " 0.36742642521858215,\n",
       " 0.40023073554039,\n",
       " 0.35841459035873413,\n",
       " 0.4365806579589844,\n",
       " 0.3787003755569458,\n",
       " 0.37915152311325073,\n",
       " 0.3731629550457001,\n",
       " 0.42876896262168884,\n",
       " 0.357844740152359,\n",
       " 0.2911759912967682,\n",
       " 0.4139937460422516,\n",
       " 0.4087449908256531,\n",
       " 0.43766841292381287,\n",
       " 0.3806232213973999,\n",
       " 0.4303228259086609,\n",
       " 0.39631637930870056,\n",
       " 0.3568074703216553,\n",
       " 0.4232620298862457,\n",
       " 0.41648074984550476,\n",
       " 0.43370184302330017,\n",
       " 0.445819616317749,\n",
       " 0.43242567777633667,\n",
       " 0.3970872461795807,\n",
       " 0.4215634763240814,\n",
       " 0.46063491702079773,\n",
       " 0.37822580337524414,\n",
       " 0.4310294985771179,\n",
       " 0.467428058385849,\n",
       " 0.3299258351325989,\n",
       " 0.42043542861938477,\n",
       " 0.3150222897529602,\n",
       " 0.3731541335582733,\n",
       " 0.3659171164035797,\n",
       " 0.369403600692749,\n",
       " 0.4220753312110901,\n",
       " 0.3852669596672058,\n",
       " 0.3697865903377533,\n",
       " 0.38614824414253235,\n",
       " 0.36827459931373596,\n",
       " 0.36674875020980835,\n",
       " 0.39814820885658264,\n",
       " 0.406708300113678,\n",
       " 0.3940671384334564,\n",
       " 0.36363980174064636,\n",
       " 0.4018438756465912,\n",
       " 0.44245147705078125,\n",
       " 0.37958571314811707,\n",
       " 0.34490257501602173,\n",
       " 0.3854883313179016,\n",
       " 0.3931420147418976,\n",
       " 0.38707438111305237,\n",
       " 0.427669495344162,\n",
       " 0.3782402276992798,\n",
       " 0.35145461559295654,\n",
       " 0.38602057099342346,\n",
       " 0.3903358280658722,\n",
       " 0.3516055941581726,\n",
       " 0.3856278955936432,\n",
       " 0.31666266918182373,\n",
       " 0.4018528163433075,\n",
       " 0.4112532138824463,\n",
       " 0.34964609146118164,\n",
       " 0.37076786160469055,\n",
       " 0.4167872369289398,\n",
       " 0.39988940954208374,\n",
       " 0.39734742045402527,\n",
       " 0.4185100495815277,\n",
       " 0.3395557105541229,\n",
       " 0.34912899136543274,\n",
       " 0.4244593381881714,\n",
       " 0.39341261982917786,\n",
       " 0.3890362083911896,\n",
       " 0.41248762607574463,\n",
       " 0.3917098939418793,\n",
       " 0.45635131001472473,\n",
       " 0.38035058975219727,\n",
       " 0.37223267555236816,\n",
       " 0.4267474114894867,\n",
       " 0.38077694177627563,\n",
       " 0.36334097385406494,\n",
       " 0.36247512698173523,\n",
       " 0.341898113489151,\n",
       " 0.4753696024417877,\n",
       " 0.3723284900188446,\n",
       " 0.4033348858356476,\n",
       " 0.4019421339035034,\n",
       " 0.5081310868263245,\n",
       " 0.41100338101387024,\n",
       " 0.32222071290016174,\n",
       " 0.41361790895462036,\n",
       " 0.37694263458251953,\n",
       " 0.38460415601730347,\n",
       " 0.3667924404144287,\n",
       " 0.4222221374511719,\n",
       " 0.3971063792705536,\n",
       " 0.34987062215805054,\n",
       " 0.38463276624679565,\n",
       " 0.4290541112422943,\n",
       " 0.381156325340271,\n",
       " 0.40714144706726074,\n",
       " 0.3944053053855896,\n",
       " 0.4322909116744995,\n",
       " 0.3805086314678192,\n",
       " 0.4056566655635834,\n",
       " 0.33323153853416443,\n",
       " 0.39309126138687134,\n",
       " 0.3860108256340027,\n",
       " 0.40552815794944763,\n",
       " 0.4540855884552002,\n",
       " 0.4001486599445343,\n",
       " 0.3306409418582916,\n",
       " 0.3904270827770233,\n",
       " 0.37813034653663635,\n",
       " 0.3163566589355469,\n",
       " 0.2976856231689453,\n",
       " 0.34834423661231995,\n",
       " 0.36117783188819885,\n",
       " 0.3463732600212097,\n",
       " 0.39943158626556396,\n",
       " 0.3483890891075134,\n",
       " 0.34087491035461426,\n",
       " 0.41162675619125366,\n",
       " 0.35246819257736206,\n",
       " 0.4063849151134491,\n",
       " ...]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "430bc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "for p, g in zip(params, grads):\n",
    "    cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afb3e717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.3791940212249756\n",
      "val 2.386850595474243\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ W2 + b2 \n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d336d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mit_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
